{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "from network import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighbourEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embed = nn.Embedding(vocab_size, dimension)\n",
    "        self.linear1 = nn.Linear(2, 128)\n",
    "        self.linear2 = nn.Linear(128, dimension)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, word, position):\n",
    "        \n",
    "        embedding = self.word_embed(word)\n",
    "        \n",
    "        pos = F.relu(self.linear1(position))\n",
    "        pos = self.dropout1(pos)\n",
    "        pos = F.relu(self.linear2(pos))\n",
    "        pos = self.dropout1(pos)\n",
    "        \n",
    "        print(embedding.shape)\n",
    "        print(pos.shape)\n",
    "        concat = torch.cat((embedding, pos), dim=-1)\n",
    "        \n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, dimension, n_neighbours):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.neighbour_embeddings = []\n",
    "        self.n_neighbours = n_neighbours\n",
    "        for idx in range(self.n_neighbours):\n",
    "            self.neighbour_embeddings.append(NeighbourEmbedding(vocab_size, dimension))\n",
    "            \n",
    "    def forward(self, words, positions):\n",
    "        embedding_outputs = []\n",
    "        for idx in range(self.n_neighbours):\n",
    "            embedding_outputs.append(self.neighbour_embeddings[idx](words[:, idx], positions[:, idx, :]))\n",
    "        \n",
    "        return torch.cat(embedding_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Field Embedding (Probably One hot encoded data)\n",
    "        \n",
    "        # Candidate Embedding Layer\n",
    "        # One dense layer\n",
    "        \n",
    "        # Neighbour Word Embedding and Positional Embedding\n",
    "        # Word Embedding - Embedding layer\n",
    "        # Positional Embedding - 2 blocks of (Dense, Relu, Dropout, Dense, Relu, Dropout)\n",
    "        # N number of Neighbours\n",
    "        \n",
    "        # Each neighbour goes through a Neighbour Encoding Attention layer of its own\n",
    "        \n",
    "        # All the neighbour encodings are concatenated together into one block\n",
    "        # This concatenated encoding is concatenated with Candidate Positon Encoding for Candidate Encoding\n",
    "        \n",
    "        # This candidate encoding is merged with Field Embedding and produces a binary output score\n",
    "        \n",
    "        self.cand_embed = nn.Linear(2, 128)\n",
    "        self.field_embed = nn.Linear(2, 128)\n",
    "        \n",
    "        self.neighbour_embeddings = Embedder(vocab_size, dimension, neighbours)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, neighbours, ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_dict = {'invoice_date':0, 'invoice_no':1, 'total':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = Path.cwd()\n",
    "xmls_path = this_dir / \"dataset\" / \"xmls\"\n",
    "ocr_path = this_dir / \"dataset\" / \"tesseract_results_lstm\"\n",
    "image_path = this_dir / \"dataset\" / \"images\"\n",
    "candidate_path = this_dir / \"dataset\" / \"candidates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Annotations: 505it [00:00, 574.68it/s]\n",
      "Attaching Candidate: 100%|██████████| 505/505 [00:01<00:00, 411.01it/s]\n",
      "Attaching Neighbours:   1%|          | 4/505 [00:00<01:40,  4.98it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-2b6f373fb948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocumentsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmls_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/Representation-Learning-for-Information-Extraction/network/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xmls_path, ocr_path, image_path, candidate_path, field_dict, n_neighbour, vocab)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmls_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeighbour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_neighbour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mocr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/Representation-Learning-for-Information-Extraction/utils/Neighbour.py\u001b[0m in \u001b[0;36mattach_neighbour\u001b[0;34m(annotation, ocr_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mcad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighbours'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboth_cads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'other_candidates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mneighbours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_neighbour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                     \u001b[0mcad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighbours'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/Representation-Learning-for-Information-Extraction/utils/Neighbour.py\u001b[0m in \u001b[0;36mfind_neighbour\u001b[0;34m(cad, words, x_offset, y_offset, width, height)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0miou_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_copy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0miou_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbb_intersection_over_boxB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighbour_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/Representation-Learning-for-Information-Extraction/utils/operations.py\u001b[0m in \u001b[0;36mbb_intersection_over_boxB\u001b[0;34m(boxA, boxB)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mxA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0myA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mxB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0myB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datas = dataset.DocumentsDataset(xmls_path, ocr_path, image_path, candidate_path, field_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14947"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datas.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  0.6985,  0.0865,  1.0000,  0.0176, -0.0512,  2.0000,  0.0472,\n",
       "        -0.0512,  3.0000,  0.0936, -0.0511,  4.0000,  0.1104, -0.0143,  5.0000,\n",
       "        -0.0133, -0.0145])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(len(datas.vocab), 32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = torch.LongTensor([datas[0][0][(idx * 3) + 3] for idx in range(5)])\n",
    "cords = torch.tensor([[datas[0][0][(idx * 3) + 4], datas[0][0][(idx * 3) + 5]] for idx in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0176, -0.0512],\n",
       "        [ 0.0472, -0.0512],\n",
       "        [ 0.0936, -0.0511],\n",
       "        [ 0.1104, -0.0143],\n",
       "        [-0.0133, -0.0145]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = embedder([words[:2], words[:2]], [cords[:2], cords[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = NeighbourEmbedding(len(datas.vocab), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-4490372a7e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "embd(torch.tensor([[words[:2], words[:2]]]), torch.tensor([[cords[:2], cords[:2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "datal = data.DataLoader(datas, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, Y = next(iter(datal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  6.9847e-01,  8.6545e-02,  1.0000e+00,  1.7647e-02,\n",
       "         -5.1182e-02,  2.0000e+00,  4.7176e-02, -5.1182e-02,  3.0000e+00,\n",
       "          9.3647e-02, -5.1091e-02,  4.0000e+00,  1.1035e-01, -1.4273e-02,\n",
       "          5.0000e+00, -1.3294e-02, -1.4455e-02],\n",
       "        [ 1.0000e+00,  2.2400e-01,  9.3091e-02,  2.2000e+01, -9.0941e-02,\n",
       "         -5.6909e-02,  2.3000e+01, -3.0824e-02, -5.7909e-02,  2.4000e+01,\n",
       "          1.1176e-02, -1.4909e-02,  2.5000e+01,  2.5647e-02,  9.0909e-05,\n",
       "          2.6000e+01,  6.1176e-02,  0.0000e+00]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7647e-02, -5.1182e-02],\n",
       "         [ 4.7176e-02, -5.1182e-02],\n",
       "         [ 9.3647e-02, -5.1091e-02],\n",
       "         [ 1.1035e-01, -1.4273e-02],\n",
       "         [-1.3294e-02, -1.4455e-02]],\n",
       "\n",
       "        [[-9.0941e-02, -5.6909e-02],\n",
       "         [-3.0824e-02, -5.7909e-02],\n",
       "         [ 1.1176e-02, -1.4909e-02],\n",
       "         [ 2.5647e-02,  9.0909e-05],\n",
       "         [ 6.1176e-02,  0.0000e+00]]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0472, -0.0512],\n",
       "        [-0.0308, -0.0579]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cords[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = x.view([-1,6,3])[:,1:,:1]\n",
    "cords = x.view([-1,6,3])[:,1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.],\n",
       "         [ 2.],\n",
       "         [ 3.],\n",
       "         [ 4.],\n",
       "         [ 5.]],\n",
       "\n",
       "        [[22.],\n",
       "         [23.],\n",
       "         [24.],\n",
       "         [25.],\n",
       "         [26.]]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [22, 23, 24, 25, 26]])"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.type(torch.LongTensor).view(-1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n",
      "torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "_t = embedder(words.type(torch.LongTensor).view(-1, 5), cords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 160])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = No. of neighbours, I = Input size, B = Batch size, D = Dimension\n",
    "# I =  3 + (3 * N)\n",
    "# [B,I]\n",
    "# F = [B]\n",
    "# C = [B]\n",
    "# NE = [ 2 * D, N]\n",
    "# [B, [NE]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4]).view([4,-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from network import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = model.ReLIE(len(datas.vocab), 32, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "out = myModel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 320])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ field id, x, y,  [1,x, y]* 5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
